import torch
import torch.nn as nn
import torch.optim as optim
import torchaudio
import xml.etree.ElementTree as ET
from torch.utils.data import Dataset, DataLoader
from torch.utils.data import WeightedRandomSampler
import math
from sklearn.metrics import classification_report

###########################
# Dataset and Annotation Parsing
###########################

import torchaudio
import xml.etree.ElementTree as ET
import torch
from torch.utils.data import Dataset

import torchaudio
import xml.etree.ElementTree as ET
import torch
from torch.utils.data import Dataset

import re
import torchaudio
import torch
from torch.utils.data import Dataset


import os
import re
import torch
import torchaudio
import xml.etree.ElementTree as ET
from torch.utils.data import Dataset

import torchaudio.functional as F

import numpy as np


import os
import re
import torch
import torchaudio
from torch.utils.data import Dataset

# chunkify for memory constraints
def resample_waveform_in_chunks(waveform, orig_freq, new_freq, chunk_size=1000000):
    """
    Resample a waveform (shape: [channels, samples]) in chunks to reduce memory usage.
    
    Args:
      waveform (Tensor): Input audio tensor.
      orig_freq (int): Original sample rate.
      new_freq (int): Target sample rate.
      chunk_size (int): Number of samples per chunk.
      
    Returns:
      Tensor: The resampled waveform.
    """
    num_samples = waveform.size(1)
    out_chunks = []
    for start in range(0, num_samples, chunk_size):
        end = min(num_samples, start + chunk_size)
        chunk = waveform[:, start:end]
        chunk_resampled = F.resample(chunk, orig_freq=orig_freq, new_freq=new_freq)
        out_chunks.append(chunk_resampled)
    return torch.cat(out_chunks, dim=1)


class AudioApneaDataset(Dataset):
    def __init__(self, mic_wav_path, annotation_txt_path,
                 segment_duration=5.0, step_duration=1.0, sample_rate=24000, n_mels=40):
        """
        Args:
          mic_wav_path (str): Path to the single audio file.
          annotation_txt_path (str): Path to the text file with apnea events.
          segment_duration (float): Duration (in seconds) of each segment.
          step_duration (float): Time (in seconds) to shift the window between segments.
          sample_rate (int): Target sample rate (e.g. 24000 Hz).
          n_mels (int): Number of mel bins (e.g. 40).
        """
        self.sample_rate = sample_rate
        self.segment_duration = segment_duration
        self.step_duration = step_duration
        self.segment_samples = int(segment_duration * sample_rate)
        self.step_samples = int(step_duration * sample_rate)
        self.n_mels = n_mels

        # Load the single audio file.
        self.mic_waveform, sr = torchaudio.load(mic_wav_path)
        # Resample if necessary.
        if sr != sample_rate:
            print(f"Resampling...")
            self.mic_waveform = resample_waveform_in_chunks(self.mic_waveform, orig_freq=sr, new_freq=sample_rate)
        
        # If stereo, take only one channel.
        print(f"Check for stereo...")
        if self.mic_waveform.shape[0] > 1:
            print(f"Monofying...")
            self.mic_waveform = self.mic_waveform[0:1]
        
        self.num_samples = self.mic_waveform.shape[1]

        # Parse annotation events from the text file.
        self.apnea_events = self._parse_annotations_from_txt(annotation_txt_path)
        print("Parsed annotation events:", self.apnea_events)

        # Create the mel spectrogram transform.
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate,
            n_mels=n_mels,
            f_min=20,
            f_max=4000
        )

        # Compute number of segments using a sliding window.
        self.num_segments = ((self.num_samples - self.segment_samples) // self.step_samples) + 1

    def _parse_annotations_from_txt(self, annotation_txt_path):
        """
        Reads a text file (e.g., generated by:
          grep ObstructiveApnea annotations.rml > ObstructiveApnea.txt)
        with lines like:
          <Event Family="Respiratory" Type="ObstructiveApnea" Start="4242" Duration="10">
        and extracts events as (start, end) tuples.
        """
        events = []
        pattern = re.compile(r'Start="(?P<start>\d+(\.\d+)?)"\s+Duration="(?P<duration>\d+(\.\d+)?)"')
        with open(annotation_txt_path, 'r') as f:
            for line in f:
                m = pattern.search(line)
                if m:
                    start = float(m.group('start'))
                    duration = float(m.group('duration'))
                    events.append((start, start + duration))
        return events

    def _segment_label(self, seg_start_sec, seg_end_sec):
        # Return 1 if any apnea event overlaps the segment, else 0.
        for event_start, event_end in self.apnea_events:
            if event_start < seg_end_sec and event_end > seg_start_sec:
                return 1
        return 0

    def __len__(self):
        return self.num_segments

    def __getitem__(self, idx):
        start_sample = idx * self.step_samples
        end_sample = start_sample + self.segment_samples

        # Extract the audio segment.
        mic_seg = self.mic_waveform[:, start_sample:end_sample]

        # Compute the mel spectrogram.
        mic_mel = self.mel_transform(mic_seg).squeeze(0)  # shape: (n_mels, T)

        # Transpose to have tokens: (T, n_mels)
        mel_features = mic_mel.transpose(0, 1)

        seg_start_sec = start_sample / self.sample_rate
        seg_end_sec = seg_start_sec + self.segment_duration
        label = self._segment_label(seg_start_sec, seg_end_sec)
        label_tensor = torch.tensor(label, dtype=torch.long)

        # Optional: print debug information for segments overlapping an apnea event.
        if label_tensor.item() == 1:
            overlapping = []
            for event_start, event_end in self.apnea_events:
                if event_start < seg_end_sec and event_end > seg_start_sec:
                    overlapping.append((event_start, event_end - event_start))
            if overlapping:
                event_str = "; ".join([f"({s:.2f} sec, {d:.2f} sec)" for s, d in overlapping])
                #print(f"Segment {seg_start_sec:.2f}-{seg_end_sec:.2f} sec overlaps annotation(s): {event_str}")

        return mel_features, label_tensor


# This version computes on the fly
class AudioApneaDatasetDual(Dataset):
    def __init__(self, mic_wav_path, tracheal_wav_path, annotation_txt_path,
                 segment_duration=5.0, step_duration=1.0, sample_rate=48000, n_mels=80):
        """
        Args:
          mic_wav_path (str): Path to the mic audio file.
          tracheal_wav_path (str): Path to the tracheal audio file.
          annotation_txt_path (str): Path to the text file with ObstructiveApnea events.
          segment_duration (float): Duration (in seconds) of each segment.
          step_duration (float): Time (in seconds) to shift the window for the next segment.
          sample_rate (int): Sample rate of the audio.
          n_mels (int): Number of mel bins.
        """
        self.sample_rate = sample_rate
        self.segment_duration = segment_duration
        self.step_duration = step_duration
        self.segment_samples = int(segment_duration * sample_rate)
        self.step_samples = int(step_duration * sample_rate)
        self.n_mels = n_mels

        # Load the two recordings.
        self.mic_waveform, sr1 = torchaudio.load(mic_wav_path)
        self.tracheal_waveform, sr2 = torchaudio.load(tracheal_wav_path)
        # assert sr1 == sample_rate and sr2 == sample_rate, "Unexpected sample rate."
        # Resample if needed.
        if sr1 != sample_rate:
            print(f"Resampling...")
            resampler = torchaudio.transforms.Resample(orig_freq=sr1, new_freq=sample_rate)
            self.mic_waveform = resampler(self.mic_waveform)

        if sr2 != sample_rate:
            print(f"Resampling...")
            resampler = torchaudio.transforms.Resample(orig_freq=sr2, new_freq=sample_rate)
            self.tracheal_waveform = resampler(self.tracheal_waveform)

        # If stereo, take only one channel.
        if self.mic_waveform.shape[0] > 1:
            self.mic_waveform = self.mic_waveform[0:1]
        if self.tracheal_waveform.shape[0] > 1:
            self.tracheal_waveform = self.tracheal_waveform[0:1]

        # Ensure both recordings are the same length.
        if self.mic_waveform.shape[1] != self.tracheal_waveform.shape[1]:
            raise ValueError("Audio recordings are not the same length.")

        self.num_samples = self.mic_waveform.shape[1]
        # Use the new parser to read events from the text file.
        self.apnea_events = self._parse_annotations_from_txt(annotation_txt_path)

        # Create the mel spectrogram transform.
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate, n_mels=n_mels)

        # Compute the number of segments using a sliding window.
        self.num_segments = ((self.num_samples - self.segment_samples) // self.step_samples) + 1

    def _parse_annotations_from_txt(self, annotation_txt_path):
        """
        Reads the annotation text file (produced by grep) and extracts events.
        Assumes lines like:
          <Event Family="Respiratory" Type="ObstructiveApnea" Start="4242" Duration="10">
        """
        events = []
        # The regex looks for Start="..." and Duration="..."
        pattern = re.compile(r'Start="(?P<start>\d+(\.\d+)?)"\s+Duration="(?P<duration>\d+(\.\d+)?)"')
        with open(annotation_txt_path, 'r') as f:
            for line in f:
                m = pattern.search(line)
                if m:
                    start = float(m.group('start'))
                    duration = float(m.group('duration'))
                    events.append((start, start + duration))
        print("Parsed annotation events from text:", events)
        return events

    def _segment_label(self, seg_start_sec, seg_end_sec):
        # Returns 1 if any apnea event overlaps with the segment, else 0.
        for event_start, event_end in self.apnea_events:
            if event_start < seg_end_sec and event_end > seg_start_sec:
                return 1
        return 0

    def __len__(self):
        return self.num_segments

    def __getitem__(self, idx):
        start_sample = idx * self.step_samples
        end_sample = start_sample + self.segment_samples

        mic_seg = self.mic_waveform[:, start_sample:end_sample]
        trac_seg = self.tracheal_waveform[:, start_sample:end_sample]

        mic_mel = self.mel_transform(mic_seg).squeeze(0)    # shape: (n_mels, T)
        trac_mel = self.mel_transform(trac_seg).squeeze(0)    # shape: (n_mels, T)

        # Concatenate along the frequency dimension and transpose:
        # resulting shape: (T, 2*n_mels)
        mel_features = torch.cat([mic_mel, trac_mel], dim=0).transpose(0, 1)

        seg_start_sec = start_sample / self.sample_rate
        seg_end_sec = seg_start_sec + self.segment_duration
        label = self._segment_label(seg_start_sec, seg_end_sec)
        label_tensor = torch.tensor(label, dtype=torch.long)
        
        # Optional: print debug info if the segment overlaps an apnea event.
        if label_tensor.item() == 1:
            overlapping = []
            for event_start, event_end in self.apnea_events:
                if event_start < seg_end_sec and event_end > seg_start_sec:
                    overlapping.append((event_start, event_end - event_start))
            if overlapping:
                event_str = "; ".join([f"({s:.2f} sec, {d:.2f} sec)" for s, d in overlapping])
                #print(f"Segment {seg_start_sec:.2f}-{seg_end_sec:.2f} sec overlaps annotation(s): {event_str}")
        
        return mel_features, label_tensor



###########################
# Transformer Model for Classification
###########################

class TransformerClassifier(nn.Module):
    def __init__(self, input_dim, model_dim=256, num_heads=4, num_layers=3, dropout=0.1):
        super().__init__()
        self.input_proj = nn.Linear(input_dim, model_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, model_dim))
        self.pos_embedding = None
        
        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.classifier = nn.Linear(model_dim, 2)

    def forward(self, x):
        batch_size, seq_len, _ = x.shape
        
        # Project input to model dimension
        x = self.input_proj(x)
        
        # Prepare CLS token and prepend it
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        
        # Dynamically create positional embeddings on the same device as x
        if self.pos_embedding is None or self.pos_embedding.shape[1] != x.shape[1]:
            self.pos_embedding = nn.Parameter(torch.randn(1, x.shape[1], x.shape[2], device=x.device), requires_grad=True)
        x = x + self.pos_embedding
        
        # Transpose for the transformer encoder
        x = x.transpose(0, 1)
        encoded = self.transformer_encoder(x)
        encoded = encoded.transpose(0, 1)
        
        # Use the CLS token for classification
        cls_encoded = encoded[:, 0, :]
        logits = self.classifier(cls_encoded)
        return logits


################################
# --- Checkpoint Utilities ---
################################


def load_checkpoint(model, optimizer, checkpoint_dir="checkpoints"):
    """Loads the checkpoint with the highest epoch number from checkpoint_dir.
    Returns the checkpoint dictionary and the highest epoch number (or 0 if none found)."""
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
        return None, 0

    checkpoint_files = [f for f in os.listdir(checkpoint_dir)
                        if f.startswith("checkpoint_epoch_") and f.endswith(".pth")]
    if not checkpoint_files:
        return None, 0

    highest_epoch = 0
    latest_file = None
    for file in checkpoint_files:
        m = re.search(r'checkpoint_epoch_(\d+)\.pth', file)
        if m:
            epoch_num = int(m.group(1))
            if epoch_num > highest_epoch:
                highest_epoch = epoch_num
                latest_file = file
    if latest_file is not None:
        checkpoint_path = os.path.join(checkpoint_dir, latest_file)
        checkpoint = torch.load(checkpoint_path)
        model.load_state_dict(checkpoint["model_state_dict"], strict=False)
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        print(f"Loaded checkpoint from {checkpoint_path} (epoch {highest_epoch})")
        return checkpoint, highest_epoch
    return None, 0

def save_checkpoint(model, optimizer, epoch, checkpoint_dir="checkpoints"):
    """Saves a checkpoint to checkpoint_dir with the epoch number in the filename."""
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
    checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch}.pth")
    torch.save({
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
    }, checkpoint_path)
    print(f"Saved checkpoint: {checkpoint_path}")


###########################
# Training Loop
###########################

def train_model(model, dataloader, num_epochs=10, lr=1e-4):
    # Set up device: use GPU if available, with DataParallel if more than one GPU is found.
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    if torch.cuda.is_available() and torch.cuda.device_count() > 1:
        print(f"Using {torch.cuda.device_count()} GPUs for training.")
        model = nn.DataParallel(model)
    
    # weight the criteria because there is a big imbalance between Apnea vs NoApnea segments
    weights = torch.tensor([0.05, 0.95], device=device)
    criterion = nn.CrossEntropyLoss(weight=weights)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    # Attempt to load a previous checkpoint.
    _, start_epoch = load_checkpoint(model, optimizer, checkpoint_dir="checkpoints")
    if start_epoch > 0:
        print(f"Resuming training from epoch {start_epoch + 1}")
    else:
        start_epoch = 0

    for epoch in range(start_epoch, num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        y_true = []
        y_pred = []
        
        for mel_features, labels in dataloader:
            mel_features = mel_features.to(device)  # shape: (batch, T, feature_dim)
            labels = labels.to(device)  # shape: (batch,)
            
            optimizer.zero_grad()
            outputs = model(mel_features)  # shape: (batch, 2)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item() * mel_features.size(0)
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
        
            # Accumulate predictions and ground truth.
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

        epoch_loss = running_loss / total
        epoch_acc = correct / total
        print(f"Epoch {epoch+1}/{num_epochs} -- Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}")

        # Report the number of segments per class.
        num_apnea = sum(1 for label in y_true if label == 1)
        num_non_apnea = sum(1 for label in y_true if label == 0)
        print(f"Segment Counts -- Apnea: {num_apnea}, Non-Apnea: {num_non_apnea}")

        # Print the classification report for this epoch.
        report = classification_report(y_true, y_pred, target_names=["Non-Apnea", "Apnea"], zero_division=0)
        print("Classification Report:")
        print(report)

        # Save a checkpoint every 3 epochs.
        if (epoch + 1) % 50 == 0:
            save_checkpoint(model, optimizer, epoch+1, checkpoint_dir="checkpoints")

    return model

###########################
# Post-training stuffs
###########################

def post_training_sanity_check(dataset, model, num_samples=1000, step_duration=1.0, threshold=0.8):

    """
    Post-training sanity check:
        - Samples num_samples examples from dataset using a weighted sampler:
            weight = 0.9 for Apnea, 0.1 for No Apnea.
        - For each sample, computes a timestamp as index * step_duration,
        runs inference through the model, and prints:
            Timestamp, Annotated (Apnea/NoApnea), Inferred (Apnea/NoApnea)
    """

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    THRESHOLD = threshold

    # Build a weight list for each sample based on its label.
    sample_weights = []
    for i in range(len(dataset)):
        _, label = dataset[i]
        # Assuming label is a torch.tensor scalar.
        if label.item() == 1:
            sample_weights.append(1)
        else:
            sample_weights.append(1)
    # Create a WeightedRandomSampler that draws num_samples samples.
    sampler = torch.utils.data.WeightedRandomSampler(
        weights=sample_weights, num_samples=num_samples, replacement=True
    )

    # Counters for annotated and inferred labels.
    annotated_apnea = 0
    inferred_apnea  = 0
    annotated_noapnea = 0
    inferred_noapnea  = 0

    # counters for sensitivity and specificity
    # Initialize counters for confusion matrix
    TP = 0  # Annotated Apnea, Inferred Apnea
    FN = 0  # Annotated Apnea, Inferred NoApnea
    TN = 0  # Annotated NoApnea, Inferred NoApnea
    FP = 0  # Annotated NoApnea, Inferred Apnea

    print("\nPost-Training Sanity Check:")
    print("Timestamp(s) | Annotated | Inferred | Probabilities")
    # Iterate over the sampler (which yields indices)
    for idx in sampler:
        # idx may be a tensor; get the python integer.
        index = idx.item() if isinstance(idx, torch.Tensor) else idx
        # Retrieve sample from dataset.
        mel_features, label = dataset[index]
        annotated_label = label.item()
        # Compute a timestamp based on the index and step_duration:
        # (Assuming each segment advances by step_duration seconds)
        timestamp = index * step_duration
        # Prepare input (add batch dimension)
        input_tensor = mel_features.unsqueeze(0).to(device)
        # Run inference
        with torch.no_grad():
            logits = model(input_tensor)
            probabilities = torch.softmax(logits, dim=1)
            #print(f"probabilities...", probabilities)
            #print(f"probabilities...", probabilities[0,1].item())
            #pred_label = probabilities.argmax(dim=1).item()
            pred_label = 1 if probabilities[0, 1].item() >= THRESHOLD else 0

        annotated = "Apnea" if label.item() == 1 else "NoApnea"
        inferred  = "Apnea" if pred_label == 1 else "NoApnea"
        print(f"{timestamp:10.2f} | {annotated:9} | {inferred:9} | {probabilities[0,1].item()}")

        # Update counters based on annotated label.
        if label.item() == 1:
            annotated_apnea += 1
        else:
            annotated_noapnea += 1
        
        # Update counters based on inferred label.
        if pred_label == 1:
            inferred_apnea += 1
        else:
            inferred_noapnea += 1
   
        # Update confusion matrix counts.
        if annotated_label == 1:
            if pred_label == 1:
                TP += 1
            else:
                FN += 1
        else:  # annotated_label == 0
            if pred_label == 0:
                TN += 1
            else:
                FP += 1

    print("\nPost-Training Sanity Check Stats:")
    print(f"Annotated Apnea: {annotated_apnea}  |  Inferred Apnea: {inferred_apnea}")
    print(f"Annotated NoApnea: {annotated_noapnea}  |  Inferred NoApnea: {inferred_noapnea}")   

    # Compute metrics. Avoid division by zero.
    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0.0
    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0
    false_positive_rate = FP / (FP + TN) if (FP + TN) > 0 else 0.0
    false_negative_rate = FN / (TP + FN) if (TP + FN) > 0 else 0.0

    print("\nPost-Training Sanity Check Stats:")
    print(f"True Positives (Annotated Apnea, Inferred Apnea): {TP}")
    print(f"False Negatives (Annotated Apnea, Inferred NoApnea): {FN}")
    print(f"True Negatives (Annotated NoApnea, Inferred NoApnea): {TN}")
    print(f"False Positives (Annotated NoApnea, Inferred Apnea): {FP}")
    print(f"Sensitivity (Recall): {sensitivity:.3f}")
    print(f"Specificity: {specificity:.3f}")
    print(f"False Positive Rate: {false_positive_rate:.3f}")
    print(f"False Negative Rate: {false_negative_rate:.3f}")


##########################################
# Dump a chunk of inferred data for debug
##########################################


def dump_debug(dataset, model, start_timestamp, num_samples, threshold=0.80, output_file="debug_dump.txt"):
    """
    Dumps debug information for num_samples consecutive samples starting from start_timestamp
    to the file output_file.
    
    Each sample is assumed to be a 5-second window produced every STEP_DURATION seconds.
    For each sample, the function retrieves the annotated label, runs inference through the model,
    and writes a line in the format:
    
        timestamp | annotated | inferred | probability_of_Apnea
    
    Parameters:
      dataset: the dataset from which to retrieve samples.
      model: the trained model.
      start_timestamp: starting time in seconds.
      num_samples: number of consecutive samples to dump.
      threshold: probability threshold for tagging a sample as Apnea.
      output_file: filename for the debug dump.
    """

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # Assume STEP_DURATION is globally defined (e.g., 1.0)
    global STEP_DURATION
    start_index = int(start_timestamp / STEP_DURATION)
    
    with open(output_file, "w") as f:
        f.write("Timestamp   | Annotated | Inferred  | Prob_Apnea\n")
        f.write("-"*50 + "\n")
        for i in range(num_samples):
            idx = start_index + i
            if idx >= len(dataset):
                break
            mel_features, label_tensor = dataset[idx]
            annotated = "Apnea" if label_tensor.item() == 1 else "NoApnea"
            # Compute timestamp as (index * STEP_DURATION) in seconds.
            timestamp = idx * STEP_DURATION
            input_tensor = mel_features.unsqueeze(0).to(device)
            with torch.no_grad():
                logits = model(input_tensor)
                probabilities = torch.softmax(logits, dim=1)
                # Use threshold: only if probability for label 1 is >= threshold, inferred is Apnea.
                inferred = "Apnea" if probabilities[0, 1].item() >= threshold else "NoApnea"
            f.write(f"{timestamp:10.2f} | {annotated:9} | {inferred:9} | {probabilities[0,1].item():.4f}\n")
    print(f"Debug dump written to {output_file}")


###########################
# Checkerboard
###########################

def checkerboard_sanity_check(dataset, model, num_cols=60, num_rows=20, start_timestamp=0.0, threshold=0.80):
    """
    Samples consecutive segments from the dataset starting at start_timestamp, for a total of num_rows*num_cols segments.
    The checkerboard is arranged with num_cols columns (seconds) and num_rows rows (minutes).
    The horizontal axis is labelled in seconds (0 to 59) and the vertical axis is labelled in minutes
    (with the first row corresponding to start_timestamp, converted to minutes).
    
    Parameters:
      dataset: The training dataset.
      model: The trained model.
      num_cols: Number of columns (seconds in a minute); default 60.
      num_rows: Number of rows (minutes to display); default 20.
      start_timestamp: The starting time (in seconds) in the audio file for sampling.
      threshold: The probability threshold for tagging a segment as Apnea.
    """

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # Total number of segments to sample.
    num_samples = num_cols * num_rows
    
    # Compute starting index. (Assumes segments are generated every STEP_DURATION seconds.)
    start_index = int(start_timestamp / STEP_DURATION)
    predictions = []
    
    for idx in range(start_index, start_index + num_samples):
        if idx >= len(dataset):
            break
        mel_features, _ = dataset[idx]
        input_tensor = mel_features.unsqueeze(0).to(device)
        with torch.no_grad():
            logits = model(input_tensor)
            probs = torch.softmax(logits, dim=1)
            # Use threshold logic: only if the probability for Apnea (label 1) exceeds threshold,
            # tag as Apnea; otherwise, tag as NoApnea.
            pred_label = 1 if probs[0, 1].item() >= threshold else 0
        predictions.append(pred_label)
    
    # Pad predictions if we didn't get enough samples.
    predictions = np.array(predictions)
    if len(predictions) < num_samples:
        predictions = np.pad(predictions, (0, num_samples - len(predictions)), mode='constant', constant_values=0)
    
    # Reshape into a grid: rows correspond to minutes, columns to seconds.
    grid = predictions.reshape(num_rows, num_cols)
    
    # Create and configure the figure.
    fig_checker = plt.figure("Checkerboard Sanity Check")
    ax = fig_checker.add_subplot(111)
    cax = ax.imshow(grid, aspect="auto", cmap="RdYlGn_r", vmin=0, vmax=1)
    
    ax.set_title("Checkerboard of Inference (Thresholded)")
    ax.set_xlabel("Seconds")
    ax.set_ylabel("Minutes")
    
    # Set horizontal ticks: one tick per 10 seconds.
    ax.set_xticks(np.arange(0, num_cols, 10))
    ax.set_xticklabels(np.arange(0, num_cols, 10))
    
    # Set vertical ticks: each row represents one minute.
    # For the vertical labels, convert the starting timestamp to minutes.
    start_minute = int(start_timestamp // 60)
    ax.set_yticks(np.arange(0, num_rows, 1))
    ax.set_yticklabels([str(start_minute + i) for i in range(num_rows)])
    
    fig_checker.colorbar(cax)
    plt.show(block=True)



#################################
# Checkerboard Movie
#################################

def checkerboard_movie(dataset, model, num_rows=20, num_cols=60, start_timestamp=0.0, threshold=0.80):
    """
    Displays a checkerboard movie of inference results.
    - The grid is num_rows x num_cols (default 20 x 60 = 1200 samples).
    - It is initialized to 0.5 (neutral gray).
    - Samples are taken consecutively from the dataset starting at start_timestamp.
    - Each sample is processed and its predicted label (0 or 1) is inserted into the
      grid in row-major order, updating one "pixel" per second.
    - The display updates at 1 fps and will exit if ESC is pressed.
    - The horizontal axis is labelled in seconds (0-59) and the vertical axis is labelled
      in minutes, starting at start_timestamp/60 and incrementing by 1 each row.
    """

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    import math
    total_pixels = num_rows * num_cols
    # Initialize grid to 0.5 (gray).
    grid = np.full((num_rows, num_cols), 0.5)
    
    # Create a persistent figure.
    fig_movie, ax_movie = plt.subplots(figsize=(10, 8))
    im_movie = ax_movie.imshow(grid, aspect="auto", cmap="RdYlGn_r", vmin=0, vmax=1)
    ax_movie.set_title("Checkerboard Movie of Inference")
    ax_movie.set_xlabel("Seconds")
    ax_movie.set_ylabel("Minutes")
    
    # Set horizontal ticks: 0 to 59 seconds.
    ax_movie.set_xticks(np.arange(0, num_cols, 10))
    ax_movie.set_xticklabels(np.arange(0, num_cols, 10))
    
    # Set vertical ticks: label each row in minutes.
    start_minute = start_timestamp / 60.0
    ax_movie.set_yticks(np.arange(0, num_rows, 1))
    ax_movie.set_yticklabels([f"{start_minute + i:.2f}" for i in range(num_rows)])
    
    fig_movie.colorbar(im_movie, ax=ax_movie)
    
    # Set up exit flag and event handler.
    exit_flag = False
    def on_key(event):
        nonlocal exit_flag
        if event.key == 'escape':
            exit_flag = True
    fig_movie.canvas.mpl_connect("key_press_event", on_key)
    
    plt.ion()
    plt.show()
    
    # Calculate starting index based on start_timestamp (assuming STEP_DURATION is defined globally).
    start_index = int(start_timestamp / STEP_DURATION)
    
    for pixel in range(total_pixels):
        if exit_flag:
            print("ESC pressed. Exiting checkerboard movie.")
            break
        idx = start_index + pixel
        if idx >= len(dataset):
            break
        
        # Get sample from dataset and run inference.
        mel_features, _ = dataset[idx]
        input_tensor = mel_features.unsqueeze(0).to(device)
        with torch.no_grad():
            logits = model(input_tensor)
            probs = torch.softmax(logits, dim=1)
            # Use threshold: tag as Apnea only if probability for label 1 >= threshold.
            pred_label = 1 if probs[0, 1].item() >= threshold else 0
        
        # Determine grid cell (row, col) in row-major order.
        row = pixel // num_cols
        col = pixel % num_cols
        grid[row, col] = pred_label
        
        # Update the image data.
        im_movie.set_data(grid)
        plt.draw()
        plt.pause(1)  # Update every 1 second.
    
    plt.ioff()
    plt.show(block=True)



###################################
# Generate MP4 of the checkerboard
###################################

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import math

def generate_checkerboard_movie(dataset, model, num_rows=20, num_cols=60, start_timestamp=0.0, threshold=0.80, output_filename="checkerboard_movie.mp4"):
    """
    Generates an MP4 video of the checkerboard movie.
    
    - The grid is num_rows x num_cols (default 20x60 = 1200 samples).
    - The movie runs at 1 fps.
    - It starts sampling from the dataset at a starting index corresponding to start_timestamp (assuming segments are produced every STEP_DURATION seconds).
    - For each frame, one segment is processed and its prediction (0 or 1) is inserted in the grid in row-major order.
    - The resulting checkerboard (with 0 mapped to green and 1 to red using the 'RdYlGn_r' colormap) is saved as an MP4.
    
    Parameters:
      dataset: the dataset from which segments are drawn.
      model: the trained model.
      num_rows: number of rows in the checkerboard.
      num_cols: number of columns in the checkerboard.
      start_timestamp: starting time (in seconds) to sample from.
      threshold: probability threshold for Apnea.
      output_filename: name of the output MP4 file.
    """

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    print(f"Generating .mp4... {output_filename}")

    total_frames = num_rows * num_cols
    # Initialize the grid to 0.5 (neutral gray).
    grid = np.full((num_rows, num_cols), 0.5)
    
    # Compute the starting index (assuming segments produced every STEP_DURATION seconds).
    start_index = int(start_timestamp / STEP_DURATION)
    
    fig, ax = plt.subplots(figsize=(10, 8))
    im = ax.imshow(grid, aspect="auto", cmap="RdYlGn_r", vmin=0, vmax=1)
    
    # Set axis labels and ticks.
    ax.set_xlabel("Seconds")
    ax.set_ylabel("Minutes")
    ax.set_xticks(np.arange(0, num_cols, 10))
    ax.set_xticklabels(np.arange(0, num_cols, 10))
    start_minute = start_timestamp / 60.0
    ax.set_yticks(np.arange(0, num_rows, 1))
    ax.set_yticklabels([f"{start_minute + i:.2f}" for i in range(num_rows)])
    ax.set_title("Checkerboard Movie of Inference")
    plt.tight_layout()
    
    # Global index pointer.
    # We'll use a closure to capture the frame number.
    def update_frame(frame):
        # Compute the dataset index.
        idx = start_index + frame
        if idx >= len(dataset):
            # If we run out of samples, leave the frame unchanged.
            return [im]
        # Retrieve sample from dataset.
        mel_features, _ = dataset[idx]
        input_tensor = mel_features.unsqueeze(0).to(device)
        with torch.no_grad():
            logits = model(input_tensor)
            probs = torch.softmax(logits, dim=1)
            # Tag as Apnea only if probability for label 1 exceeds threshold.
            pred_label = 1 if probs[0, 1].item() >= threshold else 0
        # Determine row, col in row-major order.
        row = frame // num_cols
        col = frame % num_cols
        grid[row, col] = pred_label
        im.set_data(grid)
        return [im]
    
    ani = animation.FuncAnimation(fig, update_frame, frames=total_frames, interval=1000, blit=True)
    
    # Save the animation as an MP4 using FFMpegWriter.
    Writer = animation.FFMpegWriter
    writer = Writer(fps=1, metadata=dict(artist='Your Name'), bitrate=1800)
    ani.save(output_filename, writer=writer)
    print(f"Checkerboard movie saved to {output_filename}")

# Example usage:
# generate_checkerboard_movie(dataset, model, num_rows=20, num_cols=60, start_timestamp=120.0, threshold=0.80, output_filename="checkerboard_movie.mp4")


###########################
# Putting It All Together
###########################

if __name__ == '__main__':
    # Paths to the files (update these paths as needed)
    mic_wav_path = 'datasets/mic.wav'
    tracheal_wav_path = 'datasets/tracheal.wav'
    annotation_path = 'datasets/annotations.rml'  # update with your actual .rml file path
    annotation_path = 'datasets/ObstructiveApneaList.txt'

    STEP_DURATION = 1.0

    # Create the dataset and dataloader
    dataset = AudioApneaDataset(mic_wav_path, annotation_path,
                                segment_duration=5.0,step_duration=STEP_DURATION, sample_rate=24000, n_mels=40)

    # Un-unbalance Apnea vs NoApnea
    print(f"Resampling data...")
    # collect all the labels
    all_labels = []
    for i in range(len(dataset)):
        _, label = dataset[i]
        all_labels.append(label.item())
    # conver to numpy
    all_labels = np.array(all_labels)

    # get unique labels and their counts
    unique_labels, counts = np.unique(all_labels, return_counts=True)
    print("Label distribution: ", dict(zip(unique_labels, counts)))

    # compute a weight for each class as inverse of frequency
    # this generates a lot of false positives, maybe need to reconsider...shift to 50-50 and see what happens
    class_weights = 1. / counts
    class_weights = [0.5, 0.5]
    print(f"Class weights: {class_weights}")

    # create array that maps sample to its weights
    sample_weights = np.array([class_weights[int(label)] for label in all_labels])
    sample_weights = torch.from_numpy(sample_weights).float()
    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)
 
    # sampled way
    dataloader = DataLoader(dataset, batch_size=8, sampler=sampler, num_workers=4, pin_memory=True)
    # unsampled way
    #dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)
 
    print(f"Dataset contains {len(dataset)} segments")

    # Example usage:
    # Set the dataset variable using the appropriate files.
    #dataset = AudioApneaDataset(mic_wav_path, tracheal_wav_path, annotation_path,
    #                            segment_duration=5.0, step_duration=1.0, sample_rate=48000, n_mels=80)

    # Determine input feature dimension: here it is 2*n_mels
    input_dim = 40
    
    # Initialize the model
    model = TransformerClassifier(input_dim=input_dim, model_dim=256, num_heads=4, num_layers=3, dropout=0.1)
    
    # Train the model; training will use GPU(s) if available, otherwise CPU.
    # 3000 epochs gets the model to where it thinks it has finished training (but it's screwed)
    trained_model = train_model(model, dataloader, num_epochs=3000, lr=1e-4)

    # After training completes, save the model's state dictionary.
    if isinstance(trained_model, torch.nn.DataParallel):
        torch.save(trained_model.module.state_dict(), "trained_model.pth")
    else:
        torch.save(trained_model.state_dict(), "trained_model.pth")

    # Let's look at some output...
    import matplotlib.pyplot as plt
    import math

    START_TIME = 12000.0
    APNEA_THRESHOLD = 0.999
    THRESHOLD = APNEA_THRESHOLD
    STEP_DURATION = 1.0  # Ensure this is defined

    # sanity check
    print(f"Dumping to debug.txt")
    dump_debug(dataset, model, start_timestamp=START_TIME, num_samples=1200, threshold=THRESHOLD)
    post_training_sanity_check(dataset, trained_model, num_samples=100, step_duration=STEP_DURATION, threshold=THRESHOLD)
    checkerboard_sanity_check(dataset, trained_model, num_cols=60, num_rows=20, start_timestamp = START_TIME, threshold = THRESHOLD)
    # checkerboard_movie(dataset, trained_model, num_rows=1, num_cols=60, start_timestamp=START_TIME, threshold=THRESHOLD)
    generate_checkerboard_movie(dataset, trained_model, num_rows=20, num_cols=60, start_timestamp=START_TIME, threshold=THRESHOLD, output_filename="checkerboard_movie.mp4")
